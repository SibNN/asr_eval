{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore\n",
    "\n",
    "import typing\n",
    "from typing import Any, cast, Literal\n",
    "from dataclasses import dataclass\n",
    "from itertools import pairwise\n",
    "\n",
    "import gigaam\n",
    "from gigaam.model import GigaAMASR\n",
    "import jiwer\n",
    "from datasets import Dataset, load_dataset, Audio\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from asr_eval.data import Recording\n",
    "from asr_eval.align.data import Match, Token\n",
    "from asr_eval.models.gigaam import EncodeError\n",
    "from asr_eval.streaming.sender import StreamingAudioSender, BaseStreamingAudioSender\n",
    "from asr_eval.streaming.caller import receive_full_transcription\n",
    "from asr_eval.streaming.models.vosk import VoskStreaming\n",
    "from asr_eval.streaming.model import StreamingASR, TranscriptionChunk, InputChunk, OutputChunk, Signal\n",
    "from asr_eval.streaming.evaluation import RecordingStreamingEvaluation, PartialAlignment, get_partial_alignments, remap_time, StreamingASRErrorPosition\n",
    "from asr_eval.streaming.plots import partial_alignment_diagram, visualize_history, streaming_error_vs_latency_diagram\n",
    "from asr_eval.streaming.timings import get_word_timings\n",
    "from asr_eval.serializing import save_to_json, load_from_json\n",
    "from asr_eval.utils import N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gigaam_model = typing.cast(GigaAMASR, gigaam.load_model('ctc', device='cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e20cac42fc4400fa7f87ae1d9550e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# type: ignore\n",
    "\n",
    "name, split = 'bond005/podlodka_speech', 'test'\n",
    "dataset: Dataset = (\n",
    "    load_dataset(name)[split]\n",
    "    .cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    ")\n",
    "\n",
    "samples: list[Recording] = []\n",
    "\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    try:\n",
    "        samples.append(Recording.from_sample(\n",
    "            sample=dataset[i],\n",
    "            name=name,\n",
    "            split=split,\n",
    "            index=i,\n",
    "            use_gigaam=gigaam_model,\n",
    "        ))\n",
    "    except EncodeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=13 max-active=7000 lattice-beam=6\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 1 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 2 orphan components.\n",
      "LOG (VoskAPI:Collapse():nnet-utils.cc:1488) Added 1 components, removed 2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from /home/oleg/.cache/vosk/vosk-model-ru-0.42/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:279) Loading HCLG from /home/oleg/.cache/vosk/vosk-model-ru-0.42/graph/HCLG.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:297) Loading words from /home/oleg/.cache/vosk/vosk-model-ru-0.42/graph/words.txt\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:308) Loading winfo /home/oleg/.cache/vosk/vosk-model-ru-0.42/graph/phones/word_boundary.int\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:315) Loading subtract G.fst model from /home/oleg/.cache/vosk/vosk-model-ru-0.42/rescore/G.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:317) Loading CARPA model from /home/oleg/.cache/vosk/vosk-model-ru-0.42/rescore/G.carpa\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:323) Loading RNNLM model from /home/oleg/.cache/vosk/vosk-model-ru-0.42/rnnlm/final.raw\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f3962a266b4433aabbaccfc0e36c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing bond005/podlodka_speech/test/0\n",
      "Transcribed bond005/podlodka_speech/test/0: и поэтому использовать их в повседневности не получается мы вынуждены отступать\n",
      "Transcribing bond005/podlodka_speech/test/3\n",
      "Transcribed bond005/podlodka_speech/test/3: да это отсутствие долго живущие бранчей другими словами отсутствие какому- [...]\n",
      "Transcribing bond005/podlodka_speech/test/4\n",
      "Transcribed bond005/podlodka_speech/test/4: то есть мы в каждый момент времени знаем про звук ещё и какое-то такое [...]\n",
      "Transcribing bond005/podlodka_speech/test/5\n",
      "Transcribed bond005/podlodka_speech/test/5: и мне кажется абсолютно все замечали что детские крики раздражают там и ты [...]\n",
      "Transcribing bond005/podlodka_speech/test/6\n",
      "Transcribed bond005/podlodka_speech/test/6: неужто не может быть какое-то количество дискретных столбиков где каждый [...]\n",
      "Transcribing bond005/podlodka_speech/test/7\n",
      "Transcribed bond005/podlodka_speech/test/7: второй челлендж он чисто поисковая это собственно как делать этот поиск [...]\n",
      "Transcribing bond005/podlodka_speech/test/8\n",
      "Transcribed bond005/podlodka_speech/test/8: вот пришёл запрос мы для него нашли пул кандидатов треков кандидатов и на [...]\n",
      "Transcribing bond005/podlodka_speech/test/9\n",
      "Transcribed bond005/podlodka_speech/test/9: то в силу того что люди неспособны достать но особенно не профессионалы [...]\n",
      "Transcribing bond005/podlodka_speech/test/11\n",
      "Transcribed bond005/podlodka_speech/test/11: практически за насколько это был шит на прозвучит скорее бизнесово [...]\n",
      "Transcribing bond005/podlodka_speech/test/12\n",
      "Transcribed bond005/podlodka_speech/test/12: то как бы ну зачем мне смотреть и влоги прошлой версии системы там все [...]\n",
      "Transcribing bond005/podlodka_speech/test/13\n",
      "Transcribed bond005/podlodka_speech/test/13: про то как сейчас принято делать как раз такими распределённых каких-то [...]\n",
      "Transcribing bond005/podlodka_speech/test/14\n",
      "Transcribed bond005/podlodka_speech/test/14: построить запросы как раз вот между многими хвостами одновременно и по [...]\n",
      "Transcribing bond005/podlodka_speech/test/15\n",
      "Transcribed bond005/podlodka_speech/test/15: и вот такая архитектура долгое время доминировала её пытались [...]\n",
      "Transcribing bond005/podlodka_speech/test/16\n",
      "Transcribed bond005/podlodka_speech/test/16: краеугольным камнем любые алгоритмы машинного обучения является прежде [...]\n"
     ]
    }
   ],
   "source": [
    "def default_evaluation_pipeline(\n",
    "    recording: Recording,\n",
    "    asr: StreamingASR,\n",
    ") -> RecordingStreamingEvaluation:\n",
    "    assert recording.waveform is not None\n",
    "\n",
    "    evals = RecordingStreamingEvaluation()\n",
    "    evals.id = recording.hf_uid\n",
    "\n",
    "    # preparing input audio\n",
    "    match asr.audio_type:\n",
    "        case 'float':\n",
    "            audio = recording.waveform\n",
    "            array_len_per_sec = asr.sampling_rate\n",
    "        case 'int':\n",
    "            audio = (recording.waveform * 32768).astype(np.int16)\n",
    "            array_len_per_sec = asr.sampling_rate\n",
    "        case 'bytes':\n",
    "            audio = (recording.waveform * 32768).astype(np.int16).tobytes()\n",
    "            array_len_per_sec = asr.sampling_rate * 2  # x2 because of the conversion int16 -> bytes\n",
    "    \n",
    "    # predicting\n",
    "    evals.sender = StreamingAudioSender(\n",
    "        id=evals.id,\n",
    "        audio=audio,\n",
    "        array_len_per_sec=array_len_per_sec,\n",
    "        real_time_interval_sec=1 / 5,\n",
    "        speed_multiplier=1,\n",
    "        verbose=False,\n",
    "    )\n",
    "    output_chunks = receive_full_transcription(\n",
    "        asr=asr,\n",
    "        sender=evals.sender,\n",
    "        id=evals.id,\n",
    "        send_all_without_delays=True,\n",
    "    )\n",
    "\n",
    "    # processing to save the results\n",
    "    evals.cutoffs = evals.sender.get_send_times()\n",
    "    evals.input_chunks, evals.output_chunks = remap_time(\n",
    "        evals.cutoffs,\n",
    "        evals.sender.history,\n",
    "        output_chunks,\n",
    "    )\n",
    "    evals.partial_alignments = get_partial_alignments(\n",
    "        evals.input_chunks,\n",
    "        evals.output_chunks,\n",
    "        cast(list[Token], recording.transcription_words),\n",
    "        processes=1,\n",
    "        timestamps=np.arange(\n",
    "            evals.start_timestamp,\n",
    "            evals.finish_timestamp + 0.2 - 0.0001,\n",
    "            step=0.2,\n",
    "        ).tolist(),\n",
    "    )\n",
    "\n",
    "    # partial_alignment_diagram(\n",
    "    #     evals.partial_alignments,\n",
    "    #     cast(list[Token], recording.transcription_words),\n",
    "    #     audio_len=cast(float, evals.input_chunks[-1].end_time),  # TODO: may be not precise\n",
    "    #     figsize=(12, 2),\n",
    "    #     y_type='processed',\n",
    "    # )\n",
    "\n",
    "    # cleaning large arrays to save the results\n",
    "    evals.sender.audio = ''\n",
    "    evals.sender.history = []\n",
    "    for input_chunk in evals.input_chunks:\n",
    "        if input_chunk.data != Signal.FINISH:\n",
    "            input_chunk.data = ''\n",
    "        \n",
    "    return evals\n",
    "\n",
    "\n",
    "asr = VoskStreaming(model_name='vosk-model-ru-0.42', chunk_length_sec=1)\n",
    "asr.start_thread()\n",
    "for recording in tqdm(samples):\n",
    "    recording.evals = default_evaluation_pipeline(recording, asr)\n",
    "    recording.waveform = None\n",
    "asr.stop_thread()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_json(samples, 'tmp/samples.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples: list[Recording] = load_from_json('tmp/samples.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = samples[6]\n",
    "\n",
    "partial_alignment_diagram(\n",
    "    N(N(recording.evals).partial_alignments),\n",
    "    cast(list[Token], recording.transcription_words),\n",
    "    start_real_time=N(N(N(recording.evals).input_chunks)[0].put_timestamp),\n",
    "    end_real_time=N(N(N(recording.evals).output_chunks)[-1].put_timestamp),\n",
    "    figsize=(12, 12),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_error_vs_latency_diagram(sum([\n",
    "    partial_alignment.get_error_positions()\n",
    "    for recording in samples\n",
    "    for partial_alignment in recording.evals.partial_alignments # type: ignore\n",
    "], [])) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
